## (optional) vLLM installation

```bash
cd ${APP_HOME}/repos
pyenv local 3.11
python -m venv ~/venv-vllm
source ~/venv-vllm/bin/activate
pip install vllm wheel
MAX_JOBS=4 pip install flash-attn --no-build-isolation
deactivate
```

### Running vLLM

```bash
source ~/venv-vllm/bin/activate
python -m vllm.entrypoints.openai.api_server \
--port 5000 \
--download-dir /opt/openbet/inference/hf_models \
--model cognitivecomputations/dolphin-2.8-mistral-7b-v02 \
--tokenizer cognitivecomputations/dolphin-2.8-mistral-7b-v02 \
--tokenizer-mode auto \
--dtype bfloat16 \
--api-key token-abc123 \
--swap-space 24 \
--gpu-memory-utilization 0.98 \
--max-model-len 31104
deactivate
```

### Running vLLM multi-GPU

```bash
source ~/venv-vllm/bin/activate
python -m vllm.entrypoints.openai.api_server \
--port 5000 \
--download-dir /opt/openbet/inference/hf_models \
--model cognitivecomputations/dolphin-2.8-mistral-7b-v02 \
--tokenizer cognitivecomputations/dolphin-2.8-mistral-7b-v02 \
--tokenizer-mode auto \
--dtype float32 \
--api-key token-abc123 \
--swap-space 24 \
--worker-use-ray \
--tensor-parallel-size 4

deactivate
```

### Running vLLM multi-GPU Hermes

```bash
source ~/venv-vllm/bin/activate
python -m vllm.entrypoints.openai.api_server \
--port 5000 \
--download-dir /opt/openbet/inference/hf_models \
--model NousResearch/Hermes-2-Pro-Mistral-7B \
--tokenizer NousResearch/Hermes-2-Pro-Mistral-7B \
--tokenizer-mode auto \
--dtype float32 \
--api-key token-abc123 \
--swap-space 24 \
--worker-use-ray \
--tensor-parallel-size 4

deactivate
```

### Run the tests

```bash
curl http://inference.ca.obenv.net:5000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer token-abc123" \
  -d '{
     "model": "cognitivecomputations/dolphin-2.8-mistral-7b-v02",
     "messages": [{"role": "user", "content": "Holy wow, my cool AI friend from the future! This is an inference test!"}],
     "temperature": 0.7
   }'
```